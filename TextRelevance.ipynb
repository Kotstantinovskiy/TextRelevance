{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = Mystem()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for iii, name in enumerate(os.listdir(\"data/\")):\n",
    "    print(iii, \":::\", len(os.listdir(\"data/\")))\n",
    "    \n",
    "    with open(\"data/\" + name, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    for key in data:\n",
    "        if type(data) == type(list()):\n",
    "            for i, w in enumerate(data[key]):\n",
    "                #data[key][i] = lemmatizer.lemmatize(w.decode('utf-8'))\n",
    "                data[key][i] = w.decode('utf-8').replace(\"ё\", \"е\")\n",
    "    \n",
    "    with open(\"data/\" + name, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.read_csv(\"queries_new_2.txt\", header=None, sep=\"\\t\")\n",
    "sample_df = pd.read_csv(\"sample_submission.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {}\n",
    "urls_file = open(\"urls.numerate.txt\", \"r\")\n",
    "for line in urls_file:\n",
    "    urls[int(line.split(\"\\t\")[0])] = line.split(\"\\t\")[1][:-1]\n",
    "urls_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries = []\n",
    "for i, q in enumerate(queries_df.values):\n",
    "    new_query = \"\"\n",
    "    for word in word_tokenize(q[1]):\n",
    "        lemm_word = lemmatizer.lemmatize(word)\n",
    "        lemm_word = stem.lemmatize(lemm_word)\n",
    "        for w in lemm_word:\n",
    "            if len(w) == 1 and not (w.isalpha() or w.isdigit()):\n",
    "                continue\n",
    "            else:\n",
    "                if w not in (english_stopwords | russian_stopwords):\n",
    "                    new_query = new_query + w + \" \"\n",
    "    new_query = new_query.strip()\n",
    "    new_query = new_query.replace(\"ё\", \"е\")\n",
    "    new_queries.append(new_query)\n",
    "\n",
    "queries_df[1] = new_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "sample = []\n",
    "\n",
    "for q in queries_df.values:\n",
    "    queries[int(q[0])] = q[1]\n",
    "sample = sample_df.values\n",
    "sample_df = \"\"\n",
    "queries_df = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_text_1 = dict()\n",
    "words_set = set()\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in word_tokenize(query):\n",
    "        words_set.add(w)\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_text_1[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    print(i)\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in data[b\"text\"]:\n",
    "        if w.decode(\"utf-8\") in words_set:\n",
    "            icf_text_1[w.decode(\"utf-8\")] = icf_text_1[w.decode(\"utf-8\")] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_text_1.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_text_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_text_2 = dict()\n",
    "words_set = set()\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in zip(word_tokenize(query)[:-1], word_tokenize(query)[1:]):\n",
    "        words_set.add(w[0] + \" \" + w[1])\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_text_2[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    print(i)\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in zip(data[b\"text\"][:-1], data[b\"text\"][1:]):\n",
    "        w = w[0].decode(\"utf-8\") + \" \" + w[1].decode(\"utf-8\")\n",
    "        if w in words_set:\n",
    "            icf_text_2[w] = icf_text_2[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_text_2.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_text_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_text_3 = dict()\n",
    "words_set = set()\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in zip(word_tokenize(query)[:-2], word_tokenize(query)[1:-1], word_tokenize(query)[2:]):\n",
    "        words_set.add(w[0] + \" \" + w[1] + \" \" + w[2])\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_text_3[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    print(i)\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in zip(data[b\"text\"][:-2], data[b\"text\"][1:-1], data[b\"text\"][2:]):\n",
    "        w = w[0].decode(\"utf-8\") + \" \" + w[1].decode(\"utf-8\") + \" \" + w[2].decode(\"utf-8\")\n",
    "        if w in words_set:\n",
    "            icf_text_3[w] = icf_text_3[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_text_3.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_text_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_title_1 = dict()\n",
    "words_set = set()\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in word_tokenize(query):\n",
    "        words_set.add(w)\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_title_1[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in data[b\"title\"]:\n",
    "        if w.decode(\"utf-8\") in words_set:\n",
    "            icf_title_1[w.decode(\"utf-8\")] = icf_title_1[w.decode(\"utf-8\")] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_title_1.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_title_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_title_2 = dict()\n",
    "words_set = set()\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in zip(word_tokenize(query)[:-1], word_tokenize(query)[1:]):\n",
    "        words_set.add(w[0] + \" \" + w[1])\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_title_2[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    print(i)\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in zip(data[b\"title\"][:-1], data[b\"title\"][1:]):\n",
    "        w = w[0].decode(\"utf-8\") + \" \" + w[1].decode(\"utf-8\")\n",
    "        if w in words_set:\n",
    "            icf_title_2[w] = icf_title_2[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_title_2.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_title_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_title_3 = dict()\n",
    "words_set = set()\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in zip(word_tokenize(query)[:-2], word_tokenize(query)[1:-1], word_tokenize(query)[2:]):\n",
    "        words_set.add(w[0] + \" \" + w[1] + \" \" + w[2])\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_title_3[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    print(i)\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in zip(data[b\"title\"][:-2], data[b\"title\"][1:-1], data[b\"title\"][2:]):\n",
    "        w = w[0].decode(\"utf-8\") + \" \" + w[1].decode(\"utf-8\") + \" \" + w[2].decode(\"utf-8\")\n",
    "        if w in words_set:\n",
    "            icf_title_3[w] = icf_title_3[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_title_3.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_title_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_keywords_ = {}\n",
    "words_set = set()\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in word_tokenize(query):\n",
    "        words_set.add(w)\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_keywords_[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    print(i)\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in data[b\"keywords\"]:\n",
    "        if w.decode(\"utf-8\") in words_set:\n",
    "            icf_keywords_[w.decode(\"utf-8\")] = icf_keywords_[w.decode(\"utf-8\")] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_keywords.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_keywords_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_start_ = {}\n",
    "words_set = set()\n",
    "n = 3\n",
    "\n",
    "for query in queries.values():\n",
    "    for w in word_tokenize(query):\n",
    "        words_set.add(w)\n",
    "\n",
    "for w in list(words_set):\n",
    "    icf_start_[w] = 0\n",
    "\n",
    "for i, name in enumerate(os.listdir(\"data/\")):\n",
    "    tmp_dict = {}\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for w in data[b\"text\"][:int(len(data[b\"text\"])/n)]:\n",
    "        if w.decode(\"utf-8\") in words_set:\n",
    "            icf_start_[w.decode(\"utf-8\")] = icf_start_[w.decode(\"utf-8\")] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_start.pickle\", 'wb') as f:\n",
    "    pickle.dump(icf_start_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = [b\"h1\", b\"h2\", b\"h3\", b\"h4\", b\"h5\"]\n",
    "\n",
    "for name_h in list_name:\n",
    "    icf_h_1 = {}\n",
    "    words_set = set()\n",
    "\n",
    "    for query in queries.values():\n",
    "        for w in word_tokenize(query):\n",
    "            words_set.add(w)\n",
    "\n",
    "    for w in list(words_set):\n",
    "        icf_h_1[w] = 0\n",
    "\n",
    "    for i, name in enumerate(os.listdir(\"data/\")):\n",
    "        #print(i)\n",
    "        tmp_dict = {}\n",
    "        with open(\"data/\" + name, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for w in data[name_h]:\n",
    "            if w.decode(\"utf-8\") in words_set:\n",
    "                icf_h_1[w.decode(\"utf-8\")] = icf_h_1[w.decode(\"utf-8\")] + 1\n",
    "\n",
    "    with open(\"iсf_\" + name_h.decode(\"utf-8\") + \"_1.pickle\", 'wb') as f:\n",
    "        pickle.dump(icf_h_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = [b\"h1\", b\"h2\", b\"h3\", b\"h4\", b\"h5\"]\n",
    "\n",
    "for name_h in list_name:\n",
    "    icf_h_2 = {}\n",
    "    words_set = set()\n",
    "\n",
    "    for query in queries.values():\n",
    "        for w in zip(word_tokenize(query)[:-1], word_tokenize(query)[1:]):\n",
    "            words_set.add(w[0] + \" \" + w[1])\n",
    "\n",
    "    for w in list(words_set):\n",
    "        icf_h_2[w] = 0\n",
    "\n",
    "    for i, name in enumerate(os.listdir(\"data/\")):\n",
    "        tmp_dict = {}\n",
    "        with open(\"data/\" + name, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for w in zip(data[name_h][:-1], data[name_h][1:]):\n",
    "            w = w[0].decode(\"utf-8\") + \" \" + w[1].decode(\"utf-8\")\n",
    "            if w in words_set:\n",
    "                icf_h_2[w] = icf_h_2[w] + 1\n",
    "\n",
    "    with open(\"iсf_\" + name_h.decode(\"utf-8\") + \"_2.pickle\", 'wb') as f:\n",
    "        pickle.dump(icf_h_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = [b\"h1\", b\"h2\", b\"h3\", b\"h4\", b\"h5\"]\n",
    "\n",
    "for name_h in list_name:\n",
    "    icf_h_3 = {}\n",
    "    words_set = set()\n",
    "\n",
    "    for query in queries.values():\n",
    "        for w in zip(word_tokenize(query)[:-2], word_tokenize(query)[1:-1], word_tokenize(query)[2:]):\n",
    "            words_set.add(w[0] + \" \" + w[1] + \" \" + w[2])\n",
    "            print(w[0] + \" \" + w[1] + \" \" + w[2])\n",
    "\n",
    "    for w in list(words_set):\n",
    "        icf_h_3[w] = 0\n",
    "\n",
    "    for i, name in enumerate(os.listdir(\"data/\")):\n",
    "        # print(\"03l 198 03l\" in icf_h_3.keys())\n",
    "        tmp_dict = {}\n",
    "        with open(\"data/\" + name, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for w in zip(data[name_h][:-2], data[name_h][1:-1], data[name_h][2:]):\n",
    "            w = w[0].decode(\"utf-8\") + \" \" + w[1].decode(\"utf-8\") + \" \" + w[2].decode(\"utf-8\")\n",
    "            if w in words_set:\n",
    "                icf_h_3[w] = icf_h_3[w] + 1\n",
    "                \n",
    "    with open(\"iсf_\" + name_h.decode(\"utf-8\") + \"_3.pickle\", 'wb') as f:\n",
    "        pickle.dump(icf_h_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_text_1.pickle\", 'rb') as f:\n",
    "    icf_text_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_text_2.pickle\", 'rb') as f:\n",
    "    icf_text_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_text_3.pickle\", 'rb') as f:\n",
    "    icf_text_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_title_1.pickle\", 'rb') as f:\n",
    "    icf_title_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_title_2.pickle\", 'rb') as f:\n",
    "    icf_title_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_title_3.pickle\", 'rb') as f:\n",
    "    icf_title_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_h1_1.pickle\", 'rb') as f:\n",
    "    icf_h1_1 = pickle.load(f)\n",
    "with open(\"iсf_h2_1.pickle\", 'rb') as f:\n",
    "    icf_h2_1 = pickle.load(f)\n",
    "with open(\"iсf_h3_1.pickle\", 'rb') as f:\n",
    "    icf_h3_1 = pickle.load(f)\n",
    "with open(\"iсf_h4_1.pickle\", 'rb') as f:\n",
    "    icf_h4_1 = pickle.load(f)\n",
    "with open(\"iсf_h5_1.pickle\", 'rb') as f:\n",
    "    icf_h5_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_h1_2.pickle\", 'rb') as f:\n",
    "    icf_h1_2 = pickle.load(f)\n",
    "with open(\"iсf_h2_2.pickle\", 'rb') as f:\n",
    "    icf_h2_2 = pickle.load(f)\n",
    "with open(\"iсf_h3_2.pickle\", 'rb') as f:\n",
    "    icf_h3_2 = pickle.load(f)\n",
    "with open(\"iсf_h4_2.pickle\", 'rb') as f:\n",
    "    icf_h4_2 = pickle.load(f)\n",
    "with open(\"iсf_h5_2.pickle\", 'rb') as f:\n",
    "    icf_h5_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_h1_3.pickle\", 'rb') as f:\n",
    "    icf_h1_3 = pickle.load(f)\n",
    "with open(\"iсf_h2_3.pickle\", 'rb') as f:\n",
    "    icf_h2_3 = pickle.load(f)\n",
    "with open(\"iсf_h3_3.pickle\", 'rb') as f:\n",
    "    icf_h3_3 = pickle.load(f)\n",
    "with open(\"iсf_h4_3.pickle\", 'rb') as f:\n",
    "    icf_h4_3 = pickle.load(f)\n",
    "with open(\"iсf_h5_3.pickle\", 'rb') as f:\n",
    "    icf_h5_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_start.pickle\", 'rb') as f:\n",
    "    icf_start = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iсf_keywords.pickle\", 'rb') as f:\n",
    "    icf_keywords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLemms = 0\n",
    "countDocs = len(os.listdir(\"data/\"))\n",
    "for name in os.listdir(\"data/\"):\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    allLemms = allLemms + len(data[b\"text\"])\n",
    "AVGL = allLemms / countDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalLemms = 0\n",
    "for name in os.listdir(\"data/\"):\n",
    "    with open(\"data/\" + name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    TotalLemms = TotalLemms + len(data[b\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 2.0\n",
    "b = 0.75\n",
    "TotalLemms = 96342343\n",
    "AVGL = 1297.697269702725"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_ICF = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in word_tokenize(queries[s[0]]):\n",
    "        count_ = 0\n",
    "        for wt in data[b\"text\"]:\n",
    "            if wt.decode(\"utf-8\") == w:\n",
    "                count_ = count_ + 1\n",
    "        if len(data[b\"text\"]) != 0:\n",
    "            if count_ != 0:\n",
    "                tf_tmp = count_ / len(data[b\"text\"])\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "        else:\n",
    "            tf_tmp = 0\n",
    "            \n",
    "        if icf_text_1[w] != 0:\n",
    "            tmp_icf = np.log((TotalLemms-icf_text_1[w] + 0.5)/(icf_text_1[w] + 0.5))\n",
    "            if tmp_icf > 0:\n",
    "                bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        BM25_ICF[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "BM25_ICF[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_text_1.pickle\", 'wb') as f:\n",
    "    pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_ICF = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-1], word_tokenize(queries[s[0]])[1:]):\n",
    "        w = w[0] + \" \" + w[1]\n",
    "        count_ = 0\n",
    "        for wt in zip(data[b\"text\"][:-1], data[b\"text\"][1:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                count_ = count_ + 1\n",
    "        if len(data[b\"text\"]) != 0:\n",
    "            if count_ != 0:\n",
    "                tf_tmp = count_ / len(data[b\"text\"])\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "        else:\n",
    "            tf_tmp = 0\n",
    "            \n",
    "        if icf_text_2[w] != 0:\n",
    "            tmp_icf = np.log((TotalLemms-icf_text_2[w] + 0.5)/(icf_text_2[w] + 0.5))\n",
    "            if tmp_icf > 0:\n",
    "                bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        BM25_ICF[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "BM25_ICF[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_text_2.pickle\", 'wb') as f:\n",
    "    pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_ICF = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-2], word_tokenize(queries[s[0]])[1:-1], word_tokenize(queries[s[0]])[2:]):\n",
    "        w = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        count_ = 0\n",
    "        for wt in zip(data[b\"text\"][:-2], data[b\"text\"][1:-1], data[b\"text\"][2:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\") + \" \" + wt[2].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                count_ = count_ + 1\n",
    "        if len(data[b\"text\"]) != 0:\n",
    "            if count_ != 0:\n",
    "                tf_tmp = count_ / len(data[b\"text\"])\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "        else:\n",
    "            tf_tmp = 0\n",
    "            \n",
    "        if icf_text_3[w] != 0:\n",
    "            tmp_icf = np.log((TotalLemms-icf_text_3[w] + 0.5)/(icf_text_3[w] + 0.5))\n",
    "            if tmp_icf > 0:\n",
    "                bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        BM25_ICF[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "BM25_ICF[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_text_3.pickle\", 'wb') as f:\n",
    "    pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_ICF = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in word_tokenize(queries[s[0]]):\n",
    "        count_ = 0\n",
    "        for wt in data[b\"title\"]:\n",
    "            if wt.decode(\"utf-8\") == w:\n",
    "                count_ = count_ + 1\n",
    "        if len(data[b\"title\"]) != 0:\n",
    "            if count_ != 0:\n",
    "                tf_tmp = count_ / len(data[b\"title\"])\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "        else:\n",
    "            tf_tmp = 0\n",
    "            \n",
    "        if icf_title_1[w] != 0:\n",
    "            tmp_icf = np.log((TotalLemms-icf_title_1[w] + 0.5)/(icf_title_1[w] + 0.5))\n",
    "            if tmp_icf > 0:\n",
    "                bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        BM25_ICF[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "BM25_ICF[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_title_1.pickle\", 'wb') as f:\n",
    "    pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_ICF = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-1], word_tokenize(queries[s[0]])[1:]):\n",
    "        w = w[0] + \" \" + w[1]\n",
    "        count_ = 0\n",
    "        for wt in zip(data[b\"title\"][:-1], data[b\"title\"][1:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                count_ = count_ + 1\n",
    "        if len(data[b\"title\"]) != 0:\n",
    "            if count_ != 0:\n",
    "                tf_tmp = count_ / len(data[b\"title\"])\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "        else:\n",
    "            tf_tmp = 0\n",
    "            \n",
    "        if icf_title_2[w] != 0:\n",
    "            tmp_icf = np.log((TotalLemms-icf_title_2[w] + 0.5)/(icf_title_2[w] + 0.5))\n",
    "            if tmp_icf > 0:\n",
    "                bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        BM25_ICF[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "BM25_ICF[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_title_2.pickle\", 'wb') as f:\n",
    "    pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_ICF = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-2], word_tokenize(queries[s[0]])[1:-1], word_tokenize(queries[s[0]])[2:]):\n",
    "        w = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        count_ = 0\n",
    "        for wt in zip(data[b\"title\"][:-2], data[b\"title\"][1:-1], data[b\"title\"][2:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\") + \" \" + wt[2].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                count_ = count_ + 1\n",
    "        if len(data[b\"title\"]) != 0:\n",
    "            if count_ != 0:\n",
    "                tf_tmp = count_ / len(data[b\"title\"])\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "        else:\n",
    "            tf_tmp = 0\n",
    "            \n",
    "        if icf_title_3[w] != 0:\n",
    "            tmp_icf = np.log((TotalLemms-icf_title_3[w] + 0.5)/(icf_title_3[w] + 0.5))\n",
    "            if tmp_icf > 0:\n",
    "                bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        BM25_ICF[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "BM25_ICF[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_title_3.pickle\", 'wb') as f:\n",
    "    pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h1...h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_zones = [b\"h1\", b\"h2\", b\"h3\", b\"h4\", b\"h5\"]\n",
    "\n",
    "for name_zone in name_zones:\n",
    "    \n",
    "    if name_zone.decode(\"utf-8\") == \"h1\":\n",
    "        icf = icf_h1_1\n",
    "    elif name_zone.decode(\"utf-8\") == \"h2\":\n",
    "        icf = icf_h2_1\n",
    "    elif name_zone.decode(\"utf-8\") == \"h3\":\n",
    "        icf = icf_h3_1\n",
    "    elif name_zone.decode(\"utf-8\") == \"h4\":\n",
    "        icf = icf_h4_1\n",
    "    elif name_zone.decode(\"utf-8\") == \"h5\":\n",
    "        icf = icf_h5_1\n",
    "    \n",
    "    BM25_ICF = {}\n",
    "    dict_tmp = {}\n",
    "    N = len(os.listdir(\"data/\"))\n",
    "    q_tmp = sample[0][0]\n",
    "    for s in sample:\n",
    "        with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        bm25 = 0\n",
    "        print(s)\n",
    "        for w in word_tokenize(queries[s[0]]):\n",
    "            count_ = 0\n",
    "            for wt in data[name_zone]:\n",
    "                if wt.decode(\"utf-8\") == w:\n",
    "                    count_ = count_ + 1\n",
    "            if len(data[name_zone]) != 0:\n",
    "                if count_ != 0:\n",
    "                    tf_tmp = count_ / len(data[name_zone])\n",
    "                else:\n",
    "                    tf_tmp = 0\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "\n",
    "            if icf[w] != 0:\n",
    "                tmp_icf = np.log((TotalLemms-icf[w] + 0.5)/(icf[w] + 0.5))\n",
    "                if tmp_icf > 0:\n",
    "                    bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "        if q_tmp == s[0]:\n",
    "            dict_tmp[int(s[1])] = bm25\n",
    "        else:\n",
    "            BM25_ICF[q_tmp] = dict_tmp\n",
    "            q_tmp = s[0]\n",
    "            dict_tmp = {}\n",
    "    BM25_ICF[q_tmp] = dict_tmp \n",
    "    \n",
    "    with open(\"BM25_ICF_\" + name_zone.decode(\"utf-8\") + \"_1.pickle\", 'wb') as f:\n",
    "        pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_zones = [b\"h1\", b\"h2\", b\"h3\", b\"h4\", b\"h5\"]\n",
    "\n",
    "for name_zone in name_zones:\n",
    "    \n",
    "    if name_zone.decode(\"utf-8\") == \"h1\":\n",
    "        icf = icf_h1_2\n",
    "    elif name_zone.decode(\"utf-8\") == \"h2\":\n",
    "        icf = icf_h2_2\n",
    "    elif name_zone.decode(\"utf-8\") == \"h3\":\n",
    "        icf = icf_h3_2\n",
    "    elif name_zone.decode(\"utf-8\") == \"h4\":\n",
    "        icf = icf_h4_2\n",
    "    elif name_zone.decode(\"utf-8\") == \"h5\":\n",
    "        icf = icf_h5_2\n",
    "    \n",
    "    BM25_ICF = {}\n",
    "    dict_tmp = {}\n",
    "    N = len(os.listdir(\"data/\"))\n",
    "    q_tmp = sample[0][0]\n",
    "    for s in sample:\n",
    "        with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        bm25 = 0\n",
    "        for w in zip(word_tokenize(queries[s[0]])[:-1], word_tokenize(queries[s[0]])[1:]):\n",
    "            #print(w)\n",
    "            w = w[0] + \" \" + w[1]\n",
    "            count_ = 0\n",
    "            for wt in zip(data[name_zone][:-1], data[name_zone][1:]):\n",
    "                wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\")\n",
    "                if wt == w:\n",
    "                    count_ = count_ + 1\n",
    "            if len(data[name_zone]) != 0:\n",
    "                if count_ != 0:\n",
    "                    tf_tmp = count_ / len(data[name_zone])\n",
    "                else:\n",
    "                    tf_tmp = 0\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "\n",
    "            if icf[w] != 0:\n",
    "                tmp_icf = np.log((TotalLemms-icf[w] + 0.5)/(icf[w] + 0.5))\n",
    "                if tmp_icf > 0:\n",
    "                    bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "        if q_tmp == s[0]:\n",
    "            dict_tmp[int(s[1])] = bm25\n",
    "        else:\n",
    "            BM25_ICF[q_tmp] = dict_tmp\n",
    "            q_tmp = s[0]\n",
    "            dict_tmp = {}\n",
    "    BM25_ICF[q_tmp] = dict_tmp \n",
    "    \n",
    "    with open(\"BM25_ICF_\" + name_zone.decode(\"utf-8\") + \"_2.pickle\", 'wb') as f:\n",
    "        pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_zones = [b\"h1\", b\"h2\", b\"h3\", b\"h4\", b\"h5\"]\n",
    "\n",
    "for name_zone in name_zones:\n",
    "    if name_zone.decode(\"utf-8\") == \"h1\":\n",
    "        icf = icf_h1_3\n",
    "    elif name_zone.decode(\"utf-8\") == \"h2\":\n",
    "        icf = icf_h2_3\n",
    "    elif name_zone.decode(\"utf-8\") == \"h3\":\n",
    "        icf = icf_h3_3\n",
    "    elif name_zone.decode(\"utf-8\") == \"h4\":\n",
    "        icf = icf_h4_3\n",
    "    elif name_zone.decode(\"utf-8\") == \"h5\":\n",
    "        icf = icf_h5_3\n",
    "    \n",
    "    BM25_ICF = {}\n",
    "    dict_tmp = {}\n",
    "    N = len(os.listdir(\"data/\"))\n",
    "    q_tmp = sample[0][0]\n",
    "    for s in sample:\n",
    "        with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        bm25 = 0\n",
    "        for w in zip(word_tokenize(queries[s[0]])[:-2], word_tokenize(queries[s[0]])[1:-1], word_tokenize(queries[s[0]])[2:]):\n",
    "            w = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "            count_ = 0\n",
    "            for wt in zip(data[name_zone][:-2], data[name_zone][1:-1], data[name_zone][2:]):\n",
    "                wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\") + \" \" + wt[2].decode(\"utf-8\")\n",
    "                if wt == w:\n",
    "                    count_ = count_ + 1\n",
    "            if len(data[name_zone]) != 0:\n",
    "                if count_ != 0:\n",
    "                    tf_tmp = count_ / len(data[name_zone])\n",
    "                else:\n",
    "                    tf_tmp = 0\n",
    "            else:\n",
    "                tf_tmp = 0\n",
    "\n",
    "            if icf[w] != 0:\n",
    "                tmp_icf = np.log((TotalLemms-icf[w] + 0.5)/(icf[w] + 0.5))\n",
    "                if tmp_icf > 0:\n",
    "                    bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "        if q_tmp == s[0]:\n",
    "            dict_tmp[int(s[1])] = bm25\n",
    "        else:\n",
    "            BM25_ICF[q_tmp] = dict_tmp\n",
    "            q_tmp = s[0]\n",
    "            dict_tmp = {}\n",
    "    BM25_ICF[q_tmp] = dict_tmp \n",
    "    \n",
    "    with open(\"BM25_ICF_\" + name_zone.decode(\"utf-8\") + \"_3.pickle\", 'wb') as f:\n",
    "        pickle.dump(BM25_ICF, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AllWords Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALL_WORDS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    all_words_score = 0\n",
    "    len_query = len(word_tokenize(queries[s[0]]))\n",
    "    N_miss = 0\n",
    "    for w in word_tokenize(queries[s[0]]):\n",
    "        for wt in data[b\"text\"]:\n",
    "            if wt.decode(\"utf-8\") == w:\n",
    "                all_words_score = all_words_score + np.log(TotalLemms/icf_text_1[w])\n",
    "                N_miss = N_miss + 1\n",
    "                break\n",
    "    \n",
    "    all_words_score = all_words_score * (0.03 ** (len_query - N_miss))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = all_words_score\n",
    "    else:\n",
    "        ALL_WORDS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "ALL_WORDS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_text_1.pickle\", 'wb') as f:\n",
    "    pickle.dump(ALL_WORDS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    all_words_score = 0\n",
    "    len_query = len(word_tokenize(queries[s[0]]))\n",
    "    N_miss = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-1], word_tokenize(queries[s[0]])[1:]):\n",
    "        w = w[0] + \" \" + w[1]\n",
    "        for wt in zip(data[b\"text\"][:-1], data[b\"text\"][1:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                all_words_score = all_words_score + np.log(TotalLemms/icf_text_2[w])\n",
    "                N_miss = N_miss + 1\n",
    "                break\n",
    "    \n",
    "    all_words_score = all_words_score * (0.03 ** (len_query - N_miss))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = all_words_score\n",
    "    else:\n",
    "        ALL_WORDS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "ALL_WORDS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_text_2.pickle\", 'wb') as f:\n",
    "    pickle.dump(ALL_WORDS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    all_words_score = 0\n",
    "    len_query = len(word_tokenize(queries[s[0]]))\n",
    "    N_miss = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-2], word_tokenize(queries[s[0]])[1:-1], word_tokenize(queries[s[0]])[2:]):\n",
    "        #print(w)\n",
    "        w = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        for wt in zip(data[b\"text\"][:-2], data[b\"text\"][1:-1], data[b\"text\"][2:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\") + \" \" + wt[2].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                all_words_score = all_words_score + np.log(TotalLemms/icf_text_3[w])\n",
    "                N_miss = N_miss + 1\n",
    "                break\n",
    "    \n",
    "    all_words_score = all_words_score * (0.03 ** (len_query - N_miss))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = all_words_score\n",
    "    else:\n",
    "        ALL_WORDS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "ALL_WORDS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_text_3.pickle\", 'wb') as f:\n",
    "    pickle.dump(ALL_WORDS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AllWords Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    all_words_score = 0\n",
    "    len_query = len(word_tokenize(queries[s[0]]))\n",
    "    N_miss = 0\n",
    "    for w in word_tokenize(queries[s[0]]):\n",
    "        for wt in data[b\"title\"]:\n",
    "            if wt.decode(\"utf-8\") == w:\n",
    "                all_words_score = all_words_score + np.log(TotalLemms/icf_title_1[w])\n",
    "                N_miss = N_miss + 1\n",
    "                break\n",
    "    \n",
    "    all_words_score = all_words_score * (0.03 ** (len_query - N_miss))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = all_words_score\n",
    "    else:\n",
    "        ALL_WORDS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "ALL_WORDS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_title_1.pickle\", 'wb') as f:\n",
    "    pickle.dump(ALL_WORDS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    all_words_score = 0\n",
    "    len_query = len(word_tokenize(queries[s[0]]))\n",
    "    N_miss = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-1], word_tokenize(queries[s[0]])[1:]):\n",
    "        w = w[0] + \" \" + w[1]\n",
    "        for wt in zip(data[b\"title\"][:-1], data[b\"title\"][1:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                all_words_score = all_words_score + np.log(TotalLemms/icf_title_2[w])\n",
    "                N_miss = N_miss + 1\n",
    "                break\n",
    "    \n",
    "    all_words_score = all_words_score * (0.03 ** (len_query - N_miss))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = all_words_score\n",
    "    else:\n",
    "        ALL_WORDS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "ALL_WORDS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_title_2.pickle\", 'wb') as f:\n",
    "    pickle.dump(ALL_WORDS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    all_words_score = 0\n",
    "    len_query = len(word_tokenize(queries[s[0]]))\n",
    "    N_miss = 0\n",
    "    for w in zip(word_tokenize(queries[s[0]])[:-2], word_tokenize(queries[s[0]])[1:-1], word_tokenize(queries[s[0]])[2:]):\n",
    "        w = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        for wt in zip(data[b\"title\"][:-2], data[b\"title\"][1:-1], data[b\"title\"][2:]):\n",
    "            wt = wt[0].decode(\"utf-8\") + \" \" + wt[1].decode(\"utf-8\")\n",
    "            if wt == w:\n",
    "                all_words_score = all_words_score + np.log(TotalLemms/icf_title_3[w])\n",
    "                N_miss = N_miss + 1\n",
    "                break\n",
    "    \n",
    "    all_words_score = all_words_score * (0.03 ** (len_query - N_miss))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = all_words_score\n",
    "    else:\n",
    "        ALL_WORDS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "ALL_WORDS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_title_3.pickle\", 'wb') as f:\n",
    "    pickle.dump(ALL_WORDS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KeyWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in word_tokenize(queries[s[0]]):\n",
    "        count_ = 0\n",
    "        for wt in data[b\"keywords\"]:\n",
    "            if wt.decode(\"utf-8\") == w:\n",
    "                count_ = count_ + 1\n",
    "        if len(data[b\"keywords\"]) != 0:\n",
    "            tf_tmp = count_ / len(data[b\"keywords\"])\n",
    "        else:\n",
    "            tf_tmp = 0\n",
    "            \n",
    "        if icf_keywords[w] != 0:\n",
    "            tmp_icf = np.log((TotalLemms-icf_keywords[w] + 0.5)/(icf_keywords[w] + 0.5))\n",
    "            \n",
    "            if tmp_icf > 0:\n",
    "                bm25 = bm25 + tmp_icf*((tf_tmp * (k1 + 1))/(tf_tmp+k1*(1-b+b*N/AVGL)))\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        KEYWORDS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "KEYWORDS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords.pickle\", 'wb') as f:\n",
    "    pickle.dump(KEYWORDS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = {}\n",
    "dict_tmp = {}\n",
    "N = len(os.listdir(\"data/\"))\n",
    "q_tmp = sample[0][0]\n",
    "for s in sample:\n",
    "    with open(\"data/\" + str(s[1]) + \".pickle\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    bm25 = 0\n",
    "    for w in word_tokenize(queries[s[0]]):\n",
    "        if w in urls[int(s[1])] and len(w) > 2:\n",
    "            bm25 = bm25 + 1\n",
    "    if q_tmp == s[0]:\n",
    "        dict_tmp[int(s[1])] = bm25\n",
    "    else:\n",
    "        URLS[q_tmp] = dict_tmp\n",
    "        q_tmp = s[0]\n",
    "        dict_tmp = {}\n",
    "URLS[q_tmp] = dict_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"URLS.pickle\", 'wb') as f:\n",
    "    pickle.dump(URLS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_title_1.pickle\", 'rb') as f:\n",
    "    BM25_ICF_title_1 = pickle.load(f)\n",
    "with open(\"BM25_ICF_title_2.pickle\", 'rb') as f:\n",
    "    BM25_ICF_title_2 = pickle.load(f)\n",
    "with open(\"BM25_ICF_title_3.pickle\", 'rb') as f:\n",
    "    BM25_ICF_title_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_text_1.pickle\", 'rb') as f:\n",
    "    BM25_ICF_text_1 = pickle.load(f)\n",
    "with open(\"BM25_ICF_text_2.pickle\", 'rb') as f:\n",
    "    BM25_ICF_text_2 = pickle.load(f)\n",
    "with open(\"BM25_ICF_text_3.pickle\", 'rb') as f:\n",
    "    BM25_ICF_text_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_h1_1.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h1_1 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h2_1.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h2_1 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h3_1.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h3_1 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h4_1.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h4_1 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h5_1.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h5_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_h1_2.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h1_2 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h2_2.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h2_2 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h3_2.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h3_2 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h4_2.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h4_2 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h5_2.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h5_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BM25_ICF_h1_3.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h1_3 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h2_3.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h2_3 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h3_3.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h3_3 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h4_3.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h4_3 = pickle.load(f)\n",
    "with open(\"BM25_ICF_h5_3.pickle\", 'rb') as f:\n",
    "    BM25_ICF_h5_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_text_1.pickle\", 'rb') as f:\n",
    "    ALL_WORDS_text_1 = pickle.load(f)\n",
    "with open(\"ALL_WORDS_text_2.pickle\", 'rb') as f:\n",
    "    ALL_WORDS_text_2 = pickle.load(f)\n",
    "with open(\"ALL_WORDS_text_3.pickle\", 'rb') as f:\n",
    "    ALL_WORDS_text_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ALL_WORDS_title_1.pickle\", 'rb') as f:\n",
    "    ALL_WORDS_title_1 = pickle.load(f)\n",
    "with open(\"ALL_WORDS_title_2.pickle\", 'rb') as f:\n",
    "    ALL_WORDS_title_2 = pickle.load(f)\n",
    "with open(\"ALL_WORDS_title_3.pickle\", 'rb') as f:\n",
    "    ALL_WORDS_title_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"URLS.pickle\", 'rb') as f:\n",
    "    URLS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords.pickle\", 'rb') as f:\n",
    "    KEYWORDS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25 ICF h1+h2+... Text+Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_tmp = {}\n",
    "for key1 in BM25_ICF_text_1:\n",
    "    dict_tmp = {}\n",
    "    for key2 in BM25_ICF_text_1[key1]:\n",
    "        dict_tmp[key2] = BM25_ICF_text_1[key1][key2] + 4*BM25_ICF_title_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_text_2[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 2*BM25_ICF_text_2[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_text_3[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 2.5*BM25_ICF_text_3[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_title_2[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 1.5*BM25_ICF_title_2[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_title_3[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.5*BM25_ICF_title_3[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h1_1[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 3*BM25_ICF_h1_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h2_1[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 2.5*BM25_ICF_h2_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h3_1[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 2*BM25_ICF_h3_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h4_1[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 1.5*BM25_ICF_h4_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h5_1[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 1.0*BM25_ICF_h5_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h1_2[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 2*BM25_ICF_h1_2[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h2_2[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 2*BM25_ICF_h2_2[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h1_3[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.25*BM25_ICF_h1_3[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in BM25_ICF_h2_3[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.05*BM25_ICF_h2_3[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in ALL_WORDS_text_1[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.1*ALL_WORDS_text_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in ALL_WORDS_title_1[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 1.1*ALL_WORDS_title_1[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in ALL_WORDS_text_2[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.7*ALL_WORDS_text_2[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in ALL_WORDS_title_2[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.1*ALL_WORDS_title_2[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in KEYWORDS[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.25*KEYWORDS[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "    \n",
    "for key1 in BM25_tmp:\n",
    "    dict_tmp = BM25_tmp[key1]\n",
    "    for key2 in URLS[key1]:\n",
    "        dict_tmp[key2] = BM25_tmp[key1][key2] + 0.1*URLS[key1][key2]\n",
    "    BM25_tmp[key1] = dict_tmp\n",
    "\n",
    "for key in BM25_tmp:\n",
    "    BM25_tmp[key] = sorted(BM25_tmp[key].items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_final = BM25_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = open(\"subm.txt\", \"w\")\n",
    "subm.write(\"QueryId,DocumentId\\n\")\n",
    "for score_ind in scores_final:\n",
    "    for i in scores_final[score_ind]:\n",
    "        subm.write(str(score_ind) + \",\" + str(i[0]) + \"\\n\")\n",
    "subm.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
